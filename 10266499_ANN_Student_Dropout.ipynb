{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c010bdde",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dbs.ie/images/default-source/logos/dbs-logo-2019-small.png\" align = left/>\n",
    "\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1696ddfa",
   "metadata": {},
   "source": [
    "# Predicting Student Dropout Using A.N.N.\n",
    "Capstone Project\n",
    "\n",
    "Claire Connaughton (10266499)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43992e6f",
   "metadata": {},
   "source": [
    "# Import Relevant Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93688e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "# Supress unnecessary warnings so that presentation looks clean\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc87e4c7",
   "metadata": {},
   "source": [
    "# Prepare dataset for Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f04c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataset\n",
    "\n",
    "dataset= pd.read_csv('oulad_modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6455dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all of the columns which were discarded in the EDA stage\n",
    "\n",
    "dataset.drop(columns=['student_failed'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3542d890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dataset shape\n",
    "\n",
    "print(\"This dataset is consisted of\",dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8cae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b8e7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test set\n",
    "\n",
    "train,test = train_test_split(dataset, test_size=0.3, random_state=42)\n",
    "print(\"Training Data :\",train.shape)\n",
    "print(\"Testing Data :\",test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788ce1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rename train and test sets\n",
    "train_data = train\n",
    "test_data = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b7126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat categorical variables and normalise continuous variables\n",
    "\n",
    "# Set encoding and scaling instructions\n",
    "column_transform = make_column_transformer(\n",
    "    (OneHotEncoder(), ['code_module', 'code_presentation', 'gender', 'region', 'age_band', 'disability']),\n",
    "    (OrdinalEncoder(), ['highest_education', 'imd_band']),\n",
    "    (RobustScaler(), ['num_of_prev_attempts', 'studied_credits', 'total_click', 'late_rate'])\n",
    ")\n",
    "\n",
    "# Apply column transformer to features\n",
    "train_encoded = column_transform.fit_transform(train_data)\n",
    "test_encoded = column_transform.fit_transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae37a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the treated numpy array into a dataframe\n",
    "train_x= pd.DataFrame(train_encoded)\n",
    "test_x = pd.DataFrame(test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbb5d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the target variable - dropout - from the training set\n",
    "\n",
    "# Training Data\n",
    "train_x = train_x.iloc[:,:18]\n",
    "print(\"Training Data :\", train_x.shape)\n",
    "\n",
    "# Testing Data\n",
    "test_x = test_x.iloc[:,:18]\n",
    "print(\"Testing Data :\", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec13ceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just retain the target variable - dropout\n",
    "\n",
    "# Training Data\n",
    "train_y = train_data.iloc[:,18:]\n",
    "print(\"Training Data :\", train_y.shape)\n",
    "\n",
    "# Testing Data\n",
    "test_y = test_data.iloc[:,18:]\n",
    "print(\"Testing Data :\", test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c733951",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a25f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the tensorflow environment \n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0099c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use placeholder to put and change values while the program is running.\n",
    "# For X, a place must have 18 columns, since wbcd data has 18 features.\n",
    "# For Y, a place must have 1 columns, since the target has 1 feature.\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None,18])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1827775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Weight, Bias value with randomly\n",
    "# Set W(weight) as [18,1] to account for 18 features and 1 target\n",
    "# Set b(bias)as [1] because the target 1 layers.\n",
    "\n",
    "# weight\n",
    "W = tf.Variable(tf.random_normal([18,1], seed=0), name='weight')\n",
    "\n",
    "# bias\n",
    "b = tf.Variable(tf.random_normal([1], seed=0), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd148c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.matmul(X,W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70741835",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = tf.nn.sigmoid(logits)\n",
    "\n",
    "cost_i = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=Y)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "# cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6549380",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211995fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare : original vs. prediction\n",
    "\n",
    "prediction = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "correct_prediction = tf.equal(prediction, Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c997c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate Model\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: train_x, Y: train_y})\n",
    "        if step % 1000 == 0:\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={X: train_x, Y: train_y})\n",
    "            print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, loss, acc))\n",
    "            \n",
    "    train_acc = sess.run(accuracy, feed_dict={X: train_x, Y: train_y})\n",
    "    test_acc,test_predict,test_correct = sess.run([accuracy,prediction,correct_prediction], feed_dict={X: test_x, Y: test_y})\n",
    "    print(\"Model Prediction =\", train_acc)\n",
    "    print(\"Test Prediction =\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010f7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the ANN - MLP Model\n",
    "\n",
    "def ann_mlp():\n",
    "    print(\"===========Data Summary===========\")\n",
    "    print(\"Training Data :\", train_x.shape)\n",
    "    print(\"Testing Data :\", test_x.shape)\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None,18])\n",
    "    Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    # input\n",
    "    W1 = tf.Variable(tf.random_normal([18,34], seed=0), name='weight1')\n",
    "    b1 = tf.Variable(tf.random_normal([34], seed=0), name='bias1')\n",
    "    layer1 = tf.nn.sigmoid(tf.matmul(X,W1) + b1)\n",
    "\n",
    "    # hidden1\n",
    "    W2 = tf.Variable(tf.random_normal([34,34], seed=0), name='weight2')\n",
    "    b2 = tf.Variable(tf.random_normal([34], seed=0), name='bias2')\n",
    "    layer2 = tf.nn.sigmoid(tf.matmul(layer1,W2) + b2)\n",
    "\n",
    "    # hidden2\n",
    "    W3 = tf.Variable(tf.random_normal([34,54], seed=0), name='weight3')\n",
    "    b3 = tf.Variable(tf.random_normal([54], seed=0), name='bias3')\n",
    "    layer3 = tf.nn.sigmoid(tf.matmul(layer2,W3) + b3)\n",
    "\n",
    "    # output\n",
    "    W4 = tf.Variable(tf.random_normal([54,1], seed=0), name='weight4')\n",
    "    b4 = tf.Variable(tf.random_normal([1], seed=0), name='bias4')\n",
    "    logits = tf.matmul(layer3,W4) + b4\n",
    "    hypothesis = tf.nn.sigmoid(logits)\n",
    "\n",
    "    cost_i = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=Y)\n",
    "    cost = tf.reduce_mean(cost_i)\n",
    "\n",
    "    train = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "    prediction = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "    correct_prediction = tf.equal(prediction, Y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n",
    "\n",
    "    print(\"\\n============Processing============\")\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for step in range(10001):\n",
    "            sess.run(train, feed_dict={X: train_x, Y: train_y})\n",
    "            if step % 1000 == 0:\n",
    "                loss, acc = sess.run([cost, accuracy], feed_dict={X: train_x, Y: train_y})\n",
    "                print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, loss, acc))\n",
    "\n",
    "        train_acc = sess.run(accuracy, feed_dict={X: train_x, Y: train_y})\n",
    "        test_acc,test_predict,test_correct = sess.run([accuracy,prediction,correct_prediction], feed_dict={X: test_x, Y: test_y})\n",
    "        \n",
    "        print(\"\\n============Results============\")\n",
    "        print(\"Model Prediction =\", train_acc)\n",
    "        print(\"Test Prediction =\", test_acc)\n",
    "        \n",
    "        return train_acc,test_acc\n",
    "    \n",
    "ann_mlp_train_acc, ann_mlp_test_acc = ann_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f884217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal number of pricipal components\n",
    "\n",
    "pca = sklearnPCA().fit(train_x)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf349bb",
   "metadata": {},
   "source": [
    "11 Principal Components explain 95% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c39e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ANN - MLP - PCA Model\n",
    "\n",
    "def ann_mlp_pca():\n",
    "    sklearn_pca = sklearnPCA(n_components=11)\n",
    "\n",
    "    print(\"===========Data Summary===========\")\n",
    "    pca_train_x = sklearn_pca.fit_transform(train_x)\n",
    "    print(\"PCA Training Data :\", pca_train_x.shape)\n",
    "\n",
    "    pca_test_x = sklearn_pca.fit_transform(test_x)\n",
    "    print(\"PCA Testing Data :\", pca_test_x.shape)\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None,11])\n",
    "    Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    # input\n",
    "    W1 = tf.Variable(tf.random_normal([11,33], seed=0), name='weight1')\n",
    "    b1 = tf.Variable(tf.random_normal([33], seed=0), name='bias1')\n",
    "    layer1 = tf.nn.sigmoid(tf.matmul(X,W1) + b1)\n",
    "\n",
    "    # hidden1\n",
    "    W2 = tf.Variable(tf.random_normal([33,66], seed=0), name='weight2')\n",
    "    b2 = tf.Variable(tf.random_normal([66], seed=0), name='bias2')\n",
    "    layer2 = tf.nn.sigmoid(tf.matmul(layer1,W2) + b2)\n",
    "\n",
    "    # hidden2\n",
    "    W3 = tf.Variable(tf.random_normal([66,66], seed=0), name='weight3')\n",
    "    b3 = tf.Variable(tf.random_normal([66], seed=0), name='bias3')\n",
    "    layer3 = tf.nn.sigmoid(tf.matmul(layer2,W3) + b3)\n",
    "\n",
    "    # output\n",
    "    W4 = tf.Variable(tf.random_normal([66,1], seed=0), name='weight4')\n",
    "    b4 = tf.Variable(tf.random_normal([1], seed=0), name='bias4')\n",
    "    logits = tf.matmul(layer3,W4) + b4\n",
    "    hypothesis = tf.nn.sigmoid(logits)\n",
    "\n",
    "    cost_i = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=Y)\n",
    "    cost = tf.reduce_mean(cost_i)\n",
    "\n",
    "    train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "    prediction = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "    correct_prediction = tf.equal(prediction, Y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n",
    "\n",
    "    print(\"\\n============Processing============\")\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for step in range(10001):\n",
    "            sess.run(train, feed_dict={X: pca_train_x, Y: train_y})\n",
    "            if step % 1000 == 0:\n",
    "                loss, acc = sess.run([cost, accuracy], feed_dict={X: pca_train_x, Y: train_y})\n",
    "                print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, loss, acc))\n",
    "\n",
    "        train_acc = sess.run(accuracy, feed_dict={X: pca_train_x, Y: train_y})\n",
    "        test_acc,test_predict,test_correct = sess.run([accuracy,prediction,correct_prediction], feed_dict={X: pca_test_x, Y: test_y})\n",
    "        \n",
    "        print(\"\\n============Results============\")\n",
    "        print(\"PCA Model Prediction =\", train_acc)\n",
    "        print(\"PCA Test Prediction =\", test_acc)\n",
    "        \n",
    "        return train_acc,test_acc\n",
    "        \n",
    "ann_mlp_pca_train_acc, ann_mlp_pca_test_acc = ann_mlp_pca()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03e85cc",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
